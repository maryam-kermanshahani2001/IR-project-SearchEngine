# def tokenize():
#     token = {}
#     my_normalizer = Normalizer()
#     my_tokenizer = Tokenizer()
#     for k in
#         Tokenizer.tokenize_words(my_normalizer.normalize(all_data[]))
